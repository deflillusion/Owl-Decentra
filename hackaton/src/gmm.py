# Gaussian Mixture Models –¥–ª—è –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
# –ê–≤—Ç–æ—Ä: Erik (Decentra) - –û–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.model_selection import cross_val_score
import warnings
import os
import gc
warnings.filterwarnings('ignore')

plt.style.use('default')

print("üéØ GAUSSIAN MIXTURE MODELS –î–õ–Ø –ë–ê–ù–ö–û–í–°–ö–ò–• –î–ê–ù–ù–´–•")
print("=" * 55)
print("üîç –¶–µ–ª—å: –ú—è–≥–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏")
print("‚ö° –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ + –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤")

# 1. –ó–ê–ì–†–£–ó–ö–ê –ò –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
print("\nüìä –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

data_path = "/kaggle/input/decentra"
parquet_files = []

if os.path.exists(data_path):
    for filename in os.listdir(data_path):
        if filename.endswith('.parquet'):
            parquet_files.append(os.path.join(data_path, filename))

df = None
if parquet_files:
    file_path = parquet_files[0]
    try:
        df = pd.read_parquet(file_path)
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} –∑–∞–ø–∏—Å–µ–π")

        # –î–ª—è GMM –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ - –±–æ–ª—å—à–µ —á–µ–º –¥–ª—è HDBSCAN
        MAX_CLIENTS_GMM = 30000
        unique_clients_total = df['card_id'].nunique()

        if unique_clients_total > MAX_CLIENTS_GMM:
            print(
                f"‚ö†Ô∏è –ú–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤ ({unique_clients_total:,}), —Å–æ–∑–¥–∞–µ–º –≤—ã–±–æ—Ä–∫—É: {MAX_CLIENTS_GMM:,}")

            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞
            client_summary = df.groupby('card_id').agg({
                'transaction_amount_kzt': ['count', 'sum', 'mean', 'std'],
                'transaction_timestamp': ['min', 'max'],
                'merchant_id': 'nunique',
                'mcc_category': 'nunique'
            })

            client_summary.columns = ['txn_count', 'total_amount', 'avg_amount', 'std_amount',
                                      'first_txn', 'last_txn', 'unique_merchants', 'unique_categories']

            # –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è
            client_summary['activity_level'] = pd.qcut(
                client_summary['txn_count'], q=5, labels=range(5))
            client_summary['amount_level'] = pd.qcut(
                client_summary['total_amount'], q=4, labels=range(4))
            client_summary['diversity_level'] = pd.qcut(
                client_summary['unique_merchants'], q=3, labels=range(3))

            # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞—Ç—ã
            sample_clients = []
            clients_per_strata = MAX_CLIENTS_GMM // 60  # 5*4*3 = 60 —Å—Ç—Ä–∞—Ç

            for act in range(5):
                for amt in range(4):
                    for div in range(3):
                        strata_clients = client_summary[
                            (client_summary['activity_level'] == act) &
                            (client_summary['amount_level'] == amt) &
                            (client_summary['diversity_level'] == div)
                        ].index

                        sample_size = min(clients_per_strata,
                                          len(strata_clients))
                        if sample_size > 0:
                            sample = np.random.choice(
                                strata_clients, sample_size, replace=False)
                            sample_clients.extend(sample)

            df = df[df['card_id'].isin(sample_clients)]
            print(
                f"‚úÖ –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(sample_clients):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")

            del client_summary
            gc.collect()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        df = None

if df is None:
    print("üö´ –ó–∞–≤–µ—Ä—à–∞–µ–º –∞–Ω–∞–ª–∏–∑")
    exit()

# 2. –û–ß–ò–°–¢–ö–ê –ò –û–ë–û–ì–ê–©–ï–ù–ò–ï –î–ê–ù–ù–´–•
print("\nüßπ –®–∞–≥ 2: –û—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")

original_size = len(df)
df = df[df['transaction_amount_kzt'] > 0]
df['transaction_timestamp'] = pd.to_datetime(
    df['transaction_timestamp'], errors='coerce')
df = df[df['transaction_timestamp'].notna()]

# –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
df['hour'] = df['transaction_timestamp'].dt.hour
df['day_of_week'] = df['transaction_timestamp'].dt.dayofweek
df['month'] = df['transaction_timestamp'].dt.month
df['quarter'] = df['transaction_timestamp'].dt.quarter
df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)
df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)

# –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Ä–µ–º–µ–Ω–∏
df['time_category'] = pd.cut(df['hour'],
                             bins=[0, 6, 12, 18, 24],
                             labels=['Night', 'Morning', 'Day', 'Evening'],
                             include_lowest=True)

print(f"–û—á–∏—Å—Ç–∫–∞: {original_size:,} ‚Üí {len(df):,} –∑–∞–ø–∏—Å–µ–π")

# 3. –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í –î–õ–Ø GMM
print("\nüîß –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è GMM...")

# –ë–∞–∑–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
print("  üí∞ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")
financial_features = df.groupby('card_id').agg({
    'transaction_amount_kzt': ['sum', 'mean', 'median', 'std', 'count', 'min', 'max'],
    'transaction_timestamp': ['min', 'max']
}).reset_index()

financial_features.columns = ['card_id', 'total_amount', 'avg_amount', 'median_amount',
                              'std_amount', 'transaction_count', 'min_amount', 'max_amount',
                              'first_transaction', 'last_transaction']

# –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏
print("  üõçÔ∏è –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏...")
behavioral_features = df.groupby('card_id').agg({
    'merchant_id': 'nunique',
    'merchant_city': 'nunique',
    'mcc_category': 'nunique',
    'transaction_type': 'nunique'
}).reset_index()

behavioral_features.columns = ['card_id', 'unique_merchants', 'unique_cities',
                               'unique_categories', 'unique_txn_types']

# –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
print("  ‚è∞ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã...")
time_features = df.groupby('card_id').agg({
    'hour': ['mean', 'std'],
    'day_of_week': lambda x: x.mode().iloc[0] if len(x) > 0 else 0,
    'is_weekend': 'mean',
    'is_business_hours': 'mean',
    'is_night': 'mean',
    'month': 'nunique',
    'quarter': 'nunique'
}).reset_index()

time_features.columns = ['card_id', 'avg_hour', 'hour_std', 'preferred_day',
                         'weekend_ratio', 'business_hours_ratio', 'night_ratio',
                         'active_months', 'active_quarters']

# –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ MCC (–¥–ª—è GMM –±–µ—Ä–µ–º –±–æ–ª—å—à–µ)
print("  üè™ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ–∫—É–ø–æ–∫...")
top_categories = df['mcc_category'].value_counts().head(12).index.tolist()
mcc_features = df.groupby('card_id')['mcc_category'].apply(
    lambda x: pd.Series({f'mcc_{cat.lower()}_ratio': (
        x == cat).mean() for cat in top_categories})
).reset_index()

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ GMM-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
print("  üìä –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")


def calculate_gmm_features(group):
    amounts = group['transaction_amount_kzt']
    timestamps = group['transaction_timestamp']

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    time_diffs = timestamps.diff().dt.total_seconds() / 3600
    time_diffs = time_diffs.dropna()

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    return pd.Series({
        'amount_skewness': amounts.skew(),
        'amount_kurtosis': amounts.kurtosis(),
        'amount_range_ratio': (amounts.max() - amounts.min()) / amounts.mean() if amounts.mean() > 0 else 0,
        'large_txn_ratio': (amounts > amounts.quantile(0.9)).mean(),
        'small_txn_ratio': (amounts < amounts.quantile(0.1)).mean(),
        'regularity_score': 1 / (1 + amounts.std() / amounts.mean()) if amounts.mean() > 0 else 0,
        'avg_time_between_txns': time_diffs.mean() if len(time_diffs) > 0 else 0,
        'time_consistency': 1 / (1 + time_diffs.std() / time_diffs.mean()) if len(time_diffs) > 0 and time_diffs.mean() > 0 else 0
    })


advanced_features = df.groupby('card_id').apply(
    calculate_gmm_features).reset_index()

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
print("  üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
client_features = financial_features.merge(
    behavioral_features, on='card_id', how='left')
client_features = client_features.merge(
    time_features, on='card_id', how='left')
client_features = client_features.merge(mcc_features, on='card_id', how='left')
client_features = client_features.merge(
    advanced_features, on='card_id', how='left')

# –í—ã—á–∏—Å–ª—è–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
client_features['activity_days'] = (client_features['last_transaction'] -
                                    client_features['first_transaction']).dt.days + 1
client_features['avg_daily_transactions'] = client_features['transaction_count'] / \
    client_features['activity_days']
client_features['avg_monthly_amount'] = client_features['total_amount'] / \
    (client_features['activity_days'] / 30)
client_features['coefficient_variation'] = client_features['std_amount'] / \
    client_features['avg_amount']
client_features['spending_velocity'] = client_features['total_amount'] / \
    client_features['activity_days']

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
print("üîß –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤...")
duplicates = client_features['card_id'].duplicated().sum()
if duplicates > 0:
    print(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {duplicates} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤, —É–¥–∞–ª—è–µ–º...")
    client_features = client_features.drop_duplicates(
        subset=['card_id'], keep='first')

# –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
client_features = client_features.fillna(0)
for col in client_features.select_dtypes(include=[np.number]).columns:
    if col != 'card_id':
        client_features[col] = pd.to_numeric(
            client_features[col], errors='coerce').fillna(0).astype(float)

client_features = client_features.replace([np.inf, -np.inf], 0)

# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
del df
gc.collect()

print(
    f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(client_features.columns)-1} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(client_features):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")

# 4. –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø GMM
print("\n‚öôÔ∏è –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GMM...")

# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è GMM (–≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –∫—Ä–æ–º–µ ID)
gmm_features = client_features.select_dtypes(
    include=[np.number]).columns.tolist()
gmm_features.remove('card_id')

print(f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è GMM: {len(gmm_features)}")

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–∞—Ç—Ä–∏—Ü—ã
X = client_features[gmm_features].copy()

# –ú—è–≥–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (GMM –±–æ–ª–µ–µ —É—Å—Ç–æ–π—á–∏–≤ –∫ –≤—ã–±—Ä–æ—Å–∞–º)
for col in X.columns:
    Q98 = float(X[col].quantile(0.98))
    Q02 = float(X[col].quantile(0.02))
    X.loc[X[col] > Q98, col] = Q98
    X.loc[X[col] < Q02, col] = Q02

X = X.astype(float)
print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape}")

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø GAUSSIAN MIXTURE MODEL
print("\nüéØ –®–∞–≥ 5: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è GMM...")

n_clients = len(X_scaled)
print(f"–ö–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {n_clients:,}")

# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
print("üìä –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...")

n_components_range = range(2, 16)  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—Ç 2 –¥–æ 15 –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
bic_scores = []
aic_scores = []
silhouette_scores = []
log_likelihood_scores = []

for n_comp in n_components_range:
    print(f"  –¢–µ—Å—Ç–∏—Ä—É–µ–º {n_comp} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...", end="")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü
    best_score = -np.inf
    best_model = None

    for covariance_type in ['full', 'tied', 'diag', 'spherical']:
        try:
            gmm = GaussianMixture(
                n_components=n_comp,
                covariance_type=covariance_type,
                random_state=42,
                n_init=3,
                max_iter=200
            )

            gmm.fit(X_scaled)
            score = gmm.score(X_scaled)

            if score > best_score:
                best_score = score
                best_model = gmm
        except:
            continue

    if best_model is not None:
        labels = best_model.predict(X_scaled)

        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        bic = best_model.bic(X_scaled)
        aic = best_model.aic(X_scaled)
        log_likelihood = best_model.score(X_scaled)

        # –°–∏–ª—É—ç—Ç (–Ω–∞ –≤—ã–±–æ—Ä–∫–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        if n_clients > 5000:
            sample_indices = np.random.choice(n_clients, 3000, replace=False)
            sil_score = silhouette_score(
                X_scaled[sample_indices], labels[sample_indices])
        else:
            sil_score = silhouette_score(X_scaled, labels)

        bic_scores.append(bic)
        aic_scores.append(aic)
        silhouette_scores.append(sil_score)
        log_likelihood_scores.append(log_likelihood)

        print(f" BIC: {bic:.0f}, AIC: {aic:.0f}, –°–∏–ª—É—ç—Ç: {sil_score:.3f}")
    else:
        print(" –û—à–∏–±–∫–∞")
        bic_scores.append(np.inf)
        aic_scores.append(np.inf)
        silhouette_scores.append(-1)
        log_likelihood_scores.append(-np.inf)

# –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
optimal_n_bic = n_components_range[np.argmin(bic_scores)]
optimal_n_aic = n_components_range[np.argmin(aic_scores)]
optimal_n_sil = n_components_range[np.argmax(silhouette_scores)]

print(f"\nüìä –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
print(f"‚Ä¢ –ü–æ BIC: {optimal_n_bic} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
print(f"‚Ä¢ –ü–æ AIC: {optimal_n_aic} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
print(f"‚Ä¢ –ü–æ —Å–∏–ª—É—ç—Ç—É: {optimal_n_sil} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

# –í—ã–±–∏—Ä–∞–µ–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
optimal_n = optimal_n_bic  # BIC –æ–±—ã—á–Ω–æ –ª—É—á—à–µ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
print(f"‚úÖ –í—ã–±–∏—Ä–∞–µ–º: {optimal_n} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–æ BIC)")

# 6. –§–ò–ù–ê–õ–¨–ù–ê–Ø GMM –ú–û–î–ï–õ–¨
print(f"\nüéØ –®–∞–≥ 6: –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π GMM –º–æ–¥–µ–ª–∏...")

# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
final_models = {}
for cov_type in ['full', 'tied', 'diag']:
    try:
        gmm = GaussianMixture(
            n_components=optimal_n,
            covariance_type=cov_type,
            random_state=42,
            n_init=10,
            max_iter=300
        )

        gmm.fit(X_scaled)
        final_models[cov_type] = {
            'model': gmm,
            'bic': gmm.bic(X_scaled),
            'aic': gmm.aic(X_scaled),
            'log_likelihood': gmm.score(X_scaled)
        }

        print(
            f"‚Ä¢ {cov_type}: BIC={gmm.bic(X_scaled):.0f}, AIC={gmm.aic(X_scaled):.0f}")
    except Exception as e:
        print(f"‚Ä¢ {cov_type}: –û—à–∏–±–∫–∞ - {e}")

# –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
best_cov_type = min(final_models.keys(), key=lambda x: final_models[x]['bic'])
final_gmm = final_models[best_cov_type]['model']

print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_cov_type} covariance")

# –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
final_labels = final_gmm.predict(X_scaled)
probabilities = final_gmm.predict_proba(X_scaled)
log_likelihood = final_gmm.score(X_scaled)

# –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
max_probs = probabilities.max(axis=1)
uncertainty_threshold = 0.6
uncertain_clients = (max_probs < uncertainty_threshold).sum()

print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã GMM:")
print(f"‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_n}")
print(f"‚Ä¢ –¢–∏–ø –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏: {best_cov_type}")
print(f"‚Ä¢ Log-likelihood: {log_likelihood:.2f}")
print(
    f"‚Ä¢ –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {uncertain_clients} ({uncertain_clients/n_clients*100:.1f}%)")

# –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º
client_features['gmm_cluster'] = final_labels
client_features['max_probability'] = max_probs
client_features['is_uncertain'] = (max_probs < uncertainty_threshold)

# –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
for i in range(optimal_n):
    client_features[f'prob_cluster_{i}'] = probabilities[:, i]

# 7. –ê–ù–ê–õ–ò–ó –ö–õ–ê–°–¢–ï–†–û–í
print(f"\nüìà –®–∞–≥ 7: –ê–Ω–∞–ª–∏–∑ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")

print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
cluster_sizes = pd.Series(final_labels).value_counts().sort_index()
for cluster_id, size in cluster_sizes.items():
    percentage = size / n_clients * 100
    avg_prob = probabilities[final_labels == cluster_id, cluster_id].mean()
    print(
        f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {size:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%), —Å—Ä.–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {avg_prob:.3f}")

# –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
print(f"\nüí° –ü—Ä–æ—Ñ–∏–ª–∏ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
key_metrics = ['total_amount', 'avg_amount', 'transaction_count', 'unique_merchants',
               'weekend_ratio', 'business_hours_ratio', 'regularity_score']

for cluster_id in sorted(pd.Series(final_labels).unique()):
    cluster_data = client_features[client_features['gmm_cluster'] == cluster_id]
    size = len(cluster_data)
    avg_certainty = cluster_data['max_probability'].mean()

    print(
        f"\nüîπ –ö–õ–ê–°–¢–ï–† {cluster_id} ({size:,} –∫–ª–∏–µ–Ω—Ç–æ–≤, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_certainty:.3f}):")

    for metric in key_metrics:
        if metric in cluster_data.columns:
            value = cluster_data[metric].mean()
            if 'amount' in metric:
                print(f"  ‚Ä¢ {metric}: {value:,.0f} —Ç–µ–Ω–≥–µ")
            elif 'ratio' in metric or 'score' in metric:
                print(f"  ‚Ä¢ {metric}: {value:.3f}")
            else:
                print(f"  ‚Ä¢ {metric}: {value:.1f}")

# –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
if uncertain_clients > 0:
    uncertain_data = client_features[client_features['is_uncertain']]
    print(
        f"\nüîç –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ ({uncertain_clients} –∫–ª–∏–µ–Ω—Ç–æ–≤):")
    print(
        f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ–±—â–∞—è —Å—É–º–º–∞: {uncertain_data['total_amount'].mean():,.0f} —Ç–µ–Ω–≥–µ")
    print(f"  ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —á–µ–∫: {uncertain_data['avg_amount'].mean():,.0f} —Ç–µ–Ω–≥–µ")
    print(
        f"  ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª-–≤–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π: {uncertain_data['transaction_count'].mean():.0f}")
    print(
        f"  ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {uncertain_data['max_probability'].mean():.3f}")

# 8. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\nüé® –®–∞–≥ 8: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è GMM —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

# PCA –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle(f'Gaussian Mixture Model: {optimal_n} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤', fontsize=16)

# 1. –û—Å–Ω–æ–≤–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
colors = plt.cm.tab10(np.linspace(0, 1, optimal_n))
for i in range(optimal_n):
    mask = final_labels == i
    axes[0, 0].scatter(X_pca[mask, 0], X_pca[mask, 1],
                       c=[colors[i]], label=f'–ö–ª–∞—Å—Ç–µ—Ä {i}', alpha=0.7, s=30)

axes[0, 0].set_title('GMM –∫–ª–∞—Å—Ç–µ—Ä—ã (PCA –ø—Ä–æ–µ–∫—Ü–∏—è)')
axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
axes[0, 0].legend()

# 2. –ö–∞—Ä—Ç–∞ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
scatter = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=max_probs,
                             cmap='viridis', alpha=0.6, s=20)
axes[0, 1].set_title('–ö–∞—Ä—Ç–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.colorbar(scatter, ax=axes[0, 1])

# 3. –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
axes[0, 2].bar(range(optimal_n), cluster_sizes.values, color=colors)
axes[0, 2].set_title('–†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
axes[0, 2].set_xlabel('–ö–ª–∞—Å—Ç–µ—Ä')
axes[0, 2].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤')

# 4. BIC/AIC –∫—Ä–∏–≤—ã–µ
axes[1, 0].plot(n_components_range, bic_scores, 'b-o', label='BIC')
axes[1, 0].plot(n_components_range, aic_scores, 'r-s', label='AIC')
axes[1, 0].axvline(x=optimal_n, color='green',
                   linestyle='--', label=f'–í—ã–±—Ä–∞–Ω–æ: {optimal_n}')
axes[1, 0].set_title('–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏')
axes[1, 0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
axes[1, 0].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è')
axes[1, 0].legend()

# 5. –°–∏–ª—É—ç—Ç –∞–Ω–∞–ª–∏–∑
axes[1, 1].plot(n_components_range, silhouette_scores, 'g-^')
axes[1, 1].axvline(x=optimal_n, color='green', linestyle='--')
axes[1, 1].set_title('–°–∏–ª—É—ç—Ç –∞–Ω–∞–ª–∏–∑')
axes[1, 1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
axes[1, 1].set_ylabel('–°–∏–ª—É—ç—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç')

# 6. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
axes[1, 2].hist(max_probs, bins=30, alpha=0.7, color='skyblue')
axes[1, 2].axvline(x=uncertainty_threshold, color='red', linestyle='--',
                   label=f'–ü–æ—Ä–æ–≥ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏: {uncertainty_threshold}')
axes[1, 2].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
axes[1, 2].set_xlabel('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
axes[1, 2].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤')
axes[1, 2].legend()

plt.tight_layout()
plt.show()

# 9. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\nüíæ –®–∞–≥ 9: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

# –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
output_cols = ['card_id', 'total_amount', 'avg_amount', 'transaction_count',
               'activity_days', 'unique_merchants', 'unique_cities',
               'weekend_ratio', 'business_hours_ratio', 'regularity_score',
               'gmm_cluster', 'max_probability', 'is_uncertain']

# –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–ª–±—Ü—ã —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏
for i in range(optimal_n):
    output_cols.append(f'prob_cluster_{i}')

available_output_cols = [
    col for col in output_cols if col in client_features.columns]
final_results = client_features[available_output_cols].copy()

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
assert final_results['card_id'].nunique() == len(
    final_results), "–ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã!"

final_results.to_csv('gmm_client_segments.csv', index=False)
print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'gmm_client_segments.csv'")

# –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
cluster_profiles = client_features.groupby('gmm_cluster')[gmm_features].agg([
    'mean', 'median', 'std']).round(3)
cluster_profiles.to_csv('gmm_cluster_profiles.csv')
print("‚úÖ –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'gmm_cluster_profiles.csv'")

# –°–≤–æ–¥–∫–∞ –º–æ–¥–µ–ª–∏
model_summary = {
    'algorithm': 'Gaussian Mixture Model',
    'n_clusters': optimal_n,
    'covariance_type': best_cov_type,
    'bic_score': final_models[best_cov_type]['bic'],
    'aic_score': final_models[best_cov_type]['aic'],
    'log_likelihood': log_likelihood,
    # -2 because range starts from 2
    'silhouette_score': silhouette_scores[optimal_n - 2],
    'total_clients': n_clients,
    'uncertain_clients': uncertain_clients,
    'uncertainty_percent': uncertain_clients / n_clients * 100,
    'features_used': len(gmm_features)
}

model_summary_df = pd.DataFrame([model_summary])
model_summary_df.to_csv('gmm_model_summary.csv', index=False)
print("‚úÖ –°–≤–æ–¥–∫–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'gmm_model_summary.csv'")

print(f"\nüéâ GMM –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
print("=" * 50)
print(f"üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"‚Ä¢ –ê–ª–≥–æ—Ä–∏—Ç–º: Gaussian Mixture Model")
print(f"‚Ä¢ –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_n}")
print(f"‚Ä¢ –¢–∏–ø –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏: {best_cov_type}")
print(f"‚Ä¢ BIC score: {final_models[best_cov_type]['bic']:.0f}")
