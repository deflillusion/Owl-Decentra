# –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–î–•–û–î: –ü–û–õ–ù–´–ô GMM + –ö–û–ù–¢–†–û–õ–ò–†–£–ï–ú–´–ô SpectralClustering
# –ê–≤—Ç–æ—Ä: Erik (Decentra) - –õ—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤
# –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ü–æ–ª–Ω—ã–π GMM –∞–Ω–∞–ª–∏–∑ + SpectralClustering —Ç–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.mixture import GaussianMixture
from sklearn.cluster import SpectralClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import warnings
import os
import gc
warnings.filterwarnings('ignore')

plt.style.use('default')

print("üéØ –ì–ò–ë–†–ò–î–ù–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø: –ü–û–õ–ù–´–ô GMM + –ö–û–ù–¢–†–û–õ–ò–†–£–ï–ú–´–ô SPECTRAL")
print("=" * 70)
print("üîç GMM: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∞–≤—Ç–æ–≤—ã–±–æ—Ä–æ–º (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)")
print("üîç SpectralClustering: –¢–æ–ª—å–∫–æ –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤")
print("üöÄ –¶–µ–ª—å: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ GMM + —Ç–æ—á–µ—á–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Spectral")

# === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• ===
print("\nüìä –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

data_path = "/kaggle/input/decentra"
parquet_files = []

if os.path.exists(data_path):
    for filename in os.listdir(data_path):
        if filename.endswith('.parquet'):
            parquet_files.append(os.path.join(data_path, filename))

df = None
if parquet_files:
    file_path = parquet_files[0]
    try:
        df = pd.read_parquet(file_path)
        print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]:,} –∑–∞–ø–∏—Å–µ–π")

        # –†–∞–∑—É–º–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ GMM
        MAX_CLIENTS = 20000  # –ë–æ–ª—å—à–µ –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ GMM
        unique_clients_total = df['card_id'].nunique()

        if unique_clients_total > MAX_CLIENTS:
            print(f"‚ö†Ô∏è –°–æ–∑–¥–∞–µ–º –≤—ã–±–æ—Ä–∫—É: {MAX_CLIENTS:,} –∫–ª–∏–µ–Ω—Ç–æ–≤")

            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
            client_summary = df.groupby('card_id').agg({
                'transaction_amount_kzt': ['count', 'sum', 'mean', 'std'],
                'transaction_timestamp': ['min', 'max'],
                'merchant_id': 'nunique',
                'mcc_category': 'nunique'
            })

            client_summary.columns = ['txn_count', 'total_amount', 'avg_amount', 'std_amount',
                                      'first_txn', 'last_txn', 'unique_merchants', 'unique_categories']

            # –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
            client_summary['activity_level'] = pd.qcut(
                client_summary['txn_count'], q=5, labels=range(5))
            client_summary['amount_level'] = pd.qcut(
                client_summary['total_amount'], q=4, labels=range(4))
            client_summary['diversity_level'] = pd.qcut(
                client_summary['unique_merchants'], q=3, labels=range(3))

            # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞—Ç—ã
            sample_clients = []
            clients_per_strata = MAX_CLIENTS // 60  # 5*4*3 = 60 —Å—Ç—Ä–∞—Ç

            for act in range(5):
                for amt in range(4):
                    for div in range(3):
                        strata_clients = client_summary[
                            (client_summary['activity_level'] == act) &
                            (client_summary['amount_level'] == amt) &
                            (client_summary['diversity_level'] == div)
                        ].index

                        sample_size = min(clients_per_strata,
                                          len(strata_clients))
                        if sample_size > 0:
                            sample = np.random.choice(
                                strata_clients, sample_size, replace=False)
                            sample_clients.extend(sample)

            df = df[df['card_id'].isin(sample_clients)]
            print(
                f"‚úÖ –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–∞—è —Å—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(sample_clients):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")

            del client_summary
            gc.collect()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        df = None

if df is None:
    print("üö´ –ó–∞–≤–µ—Ä—à–∞–µ–º –∞–Ω–∞–ª–∏–∑")
    exit()

# === –ü–û–õ–ù–ê–Ø –û–ß–ò–°–¢–ö–ê –ò –û–ë–û–ì–ê–©–ï–ù–ò–ï (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) ===
print("\nüßπ –®–∞–≥ 2: –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")

original_size = len(df)
df = df[df['transaction_amount_kzt'] > 0]
df['transaction_timestamp'] = pd.to_datetime(
    df['transaction_timestamp'], errors='coerce')
df = df[df['transaction_timestamp'].notna()]

# –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
df['hour'] = df['transaction_timestamp'].dt.hour
df['day_of_week'] = df['transaction_timestamp'].dt.dayofweek
df['month'] = df['transaction_timestamp'].dt.month
df['quarter'] = df['transaction_timestamp'].dt.quarter
df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)
df['is_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)

print(f"–û—á–∏—Å—Ç–∫–∞: {original_size:,} ‚Üí {len(df):,} –∑–∞–ø–∏—Å–µ–π")

# === –ü–û–õ–ù–û–ï –°–û–ó–î–ê–ù–ò–ï –ü–†–ò–ó–ù–ê–ö–û–í (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) ===
print("\nüîß –®–∞–≥ 3: –ü–æ–ª–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")

# –ë–∞–∑–æ–≤—ã–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("  üí∞ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")
financial_features = df.groupby('card_id').agg({
    'transaction_amount_kzt': ['sum', 'mean', 'median', 'std', 'count', 'min', 'max'],
    'transaction_timestamp': ['min', 'max']
}).reset_index()

financial_features.columns = ['card_id', 'total_amount', 'avg_amount', 'median_amount',
                              'std_amount', 'transaction_count', 'min_amount', 'max_amount',
                              'first_transaction', 'last_transaction']

# –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("  üõçÔ∏è –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏...")
behavioral_features = df.groupby('card_id').agg({
    'merchant_id': 'nunique',
    'merchant_city': 'nunique',
    'mcc_category': 'nunique',
    'transaction_type': 'nunique'
}).reset_index()

behavioral_features.columns = ['card_id', 'unique_merchants', 'unique_cities',
                               'unique_categories', 'unique_txn_types']

# –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("  ‚è∞ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã...")
time_features = df.groupby('card_id').agg({
    'hour': ['mean', 'std'],
    'day_of_week': lambda x: x.mode().iloc[0] if len(x) > 0 else 0,
    'is_weekend': 'mean',
    'is_business_hours': 'mean',
    'is_night': 'mean',
    'month': 'nunique',
    'quarter': 'nunique'
}).reset_index()

time_features.columns = ['card_id', 'avg_hour', 'hour_std', 'preferred_day',
                         'weekend_ratio', 'business_hours_ratio', 'night_ratio',
                         'active_months', 'active_quarters']

# –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ MCC (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("  üè™ –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ–∫—É–ø–æ–∫...")
top_categories = df['mcc_category'].value_counts().head(10).index.tolist()
mcc_features = df.groupby('card_id')['mcc_category'].apply(
    lambda x: pd.Series({f'mcc_{cat.lower()}_ratio': (
        x == cat).mean() for cat in top_categories})
).reset_index()

# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("  üìä –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–µ—Ç—Ä–∏–∫–∏...")


def calculate_advanced_features(group):
    amounts = group['transaction_amount_kzt']
    timestamps = group['transaction_timestamp']

    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
    time_diffs = timestamps.diff().dt.total_seconds() / 3600
    time_diffs = time_diffs.dropna()

    return pd.Series({
        'amount_skewness': amounts.skew(),
        'amount_kurtosis': amounts.kurtosis(),
        'amount_range_ratio': (amounts.max() - amounts.min()) / amounts.mean() if amounts.mean() > 0 else 0,
        'large_txn_ratio': (amounts > amounts.quantile(0.9)).mean(),
        'small_txn_ratio': (amounts < amounts.quantile(0.1)).mean(),
        'regularity_score': 1 / (1 + amounts.std() / amounts.mean()) if amounts.mean() > 0 else 0,
        'avg_time_between_txns': time_diffs.mean() if len(time_diffs) > 0 else 0,
        'time_consistency': 1 / (1 + time_diffs.std() / time_diffs.mean()) if len(time_diffs) > 0 and time_diffs.mean() > 0 else 0,
        'txn_density': len(amounts) / ((timestamps.max() - timestamps.min()).days + 1) if len(amounts) > 1 else 0,
        'peak_hour_concentration': group['hour'].value_counts().max() / len(group) if len(group) > 0 else 0
    })


advanced_features = df.groupby('card_id').apply(
    calculate_advanced_features).reset_index()

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("  üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
client_features = financial_features.merge(
    behavioral_features, on='card_id', how='left')
client_features = client_features.merge(
    time_features, on='card_id', how='left')
client_features = client_features.merge(mcc_features, on='card_id', how='left')
client_features = client_features.merge(
    advanced_features, on='card_id', how='left')

# –í—ã—á–∏—Å–ª—è–µ–º—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
client_features['activity_days'] = (client_features['last_transaction'] -
                                    client_features['first_transaction']).dt.days + 1
client_features['avg_daily_transactions'] = client_features['transaction_count'] / \
    client_features['activity_days']
client_features['avg_monthly_amount'] = client_features['total_amount'] / \
    (client_features['activity_days'] / 30)
client_features['coefficient_variation'] = client_features['std_amount'] / \
    client_features['avg_amount']
client_features['spending_velocity'] = client_features['total_amount'] / \
    client_features['activity_days']

# –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
client_features = client_features.fillna(0)
for col in client_features.select_dtypes(include=[np.number]).columns:
    if col != 'card_id':
        client_features[col] = pd.to_numeric(
            client_features[col], errors='coerce').fillna(0).astype(float)

client_features = client_features.replace([np.inf, -np.inf], 0)

# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
del df
gc.collect()

print(
    f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(client_features.columns)-1} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {len(client_features):,} –∫–ª–∏–µ–Ω—Ç–æ–≤")

# === –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• ===
print("\n‚öôÔ∏è –®–∞–≥ 4: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
feature_cols = client_features.select_dtypes(
    include=[np.number]).columns.tolist()
feature_cols.remove('card_id')

X = client_features[feature_cols].copy()

# –ú—è–≥–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
for col in X.columns:
    Q98 = float(X[col].quantile(0.98))
    Q02 = float(X[col].quantile(0.02))
    X.loc[X[col] > Q98, col] = Q98
    X.loc[X[col] < Q02, col] = Q02

X = X.astype(float)
print(f"–ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {X.shape}")

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# === –≠–¢–ê–ü 1: –ü–û–õ–ù–´–ô GMM –ê–ù–ê–õ–ò–ó (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) ===
print("\nüéØ –≠–¢–ê–ü 1: –ü–æ–ª–Ω—ã–π GMM –∞–Ω–∞–ª–∏–∑...")

n_clients = len(X_scaled)
print(f"–ö–ª–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {n_clients:,}")

# –ü–æ–ª–Ω—ã–π –ø–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
n_components_range = range(2, 16)
bic_scores = []
aic_scores = []
silhouette_scores = []

print("üìä –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç GMM...")
for n_comp in n_components_range:
    print(f"  –¢–µ—Å—Ç–∏—Ä—É–µ–º {n_comp} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç...", end="")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
    best_score = -np.inf
    best_model = None

    for covariance_type in ['full', 'tied', 'diag', 'spherical']:
        try:
            gmm = GaussianMixture(
                n_components=n_comp,
                covariance_type=covariance_type,
                random_state=42,
                n_init=3,
                max_iter=200
            )

            gmm.fit(X_scaled)
            score = gmm.score(X_scaled)

            if score > best_score:
                best_score = score
                best_model = gmm
        except:
            continue

    if best_model is not None:
        labels = best_model.predict(X_scaled)

        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
        bic = best_model.bic(X_scaled)
        aic = best_model.aic(X_scaled)

        # –°–∏–ª—É—ç—Ç (–Ω–∞ –≤—ã–±–æ—Ä–∫–µ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏)
        if n_clients > 5000:
            sample_indices = np.random.choice(n_clients, 3000, replace=False)
            sil_score = silhouette_score(
                X_scaled[sample_indices], labels[sample_indices])
        else:
            sil_score = silhouette_score(X_scaled, labels)

        bic_scores.append(bic)
        aic_scores.append(aic)
        silhouette_scores.append(sil_score)

        print(f" BIC: {bic:.0f}, AIC: {aic:.0f}, –°–∏–ª—É—ç—Ç: {sil_score:.3f}")
    else:
        print(" –û—à–∏–±–∫–∞")
        bic_scores.append(np.inf)
        aic_scores.append(np.inf)
        silhouette_scores.append(-1)

# –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
optimal_n_bic = n_components_range[np.argmin(bic_scores)]
optimal_n_aic = n_components_range[np.argmin(aic_scores)]
optimal_n_sil = n_components_range[np.argmax(silhouette_scores)]

print(f"\nüìä –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
print(f"‚Ä¢ –ü–æ BIC: {optimal_n_bic} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
print(f"‚Ä¢ –ü–æ AIC: {optimal_n_aic} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
print(f"‚Ä¢ –ü–æ —Å–∏–ª—É—ç—Ç—É: {optimal_n_sil} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")

# –í—ã–±–∏—Ä–∞–µ–º –∫–æ–º–ø—Ä–æ–º–∏—Å—Å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
optimal_n_gmm = optimal_n_bic  # BIC –æ–±—ã—á–Ω–æ –ª—É—á—à–µ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
print(f"‚úÖ –í—ã–±–∏—Ä–∞–µ–º: {optimal_n_gmm} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ø–æ BIC)")

# –§–∏–Ω–∞–ª—å–Ω–∞—è GMM –º–æ–¥–µ–ª—å (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print("üéØ –û–±—É—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π GMM –º–æ–¥–µ–ª–∏...")
final_models = {}
for cov_type in ['full', 'tied', 'diag']:
    try:
        gmm = GaussianMixture(
            n_components=optimal_n_gmm,
            covariance_type=cov_type,
            random_state=42,
            n_init=10,
            max_iter=300
        )

        gmm.fit(X_scaled)
        final_models[cov_type] = {
            'model': gmm,
            'bic': gmm.bic(X_scaled),
            'aic': gmm.aic(X_scaled),
            'log_likelihood': gmm.score(X_scaled)
        }

        print(
            f"‚Ä¢ {cov_type}: BIC={gmm.bic(X_scaled):.0f}, AIC={gmm.aic(X_scaled):.0f}")
    except Exception as e:
        print(f"‚Ä¢ {cov_type}: –û—à–∏–±–∫–∞ - {e}")

# –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
best_cov_type = min(final_models.keys(), key=lambda x: final_models[x]['bic'])
final_gmm = final_models[best_cov_type]['model']

print(f"‚úÖ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_cov_type} covariance")

# –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
gmm_labels = final_gmm.predict(X_scaled)
gmm_probabilities = final_gmm.predict_proba(X_scaled)
gmm_max_probs = gmm_probabilities.max(axis=1)

# –ê–Ω–∞–ª–∏–∑ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
uncertainty_threshold = 0.6
uncertain_clients = (gmm_max_probs < uncertainty_threshold).sum()

print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã GMM:")
print(f"‚Ä¢ –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_n_gmm}")
print(f"‚Ä¢ –¢–∏–ø –∫–æ–≤–∞—Ä–∏–∞—Ü–∏–∏: {best_cov_type}")
print(
    f"‚Ä¢ –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {uncertain_clients} ({uncertain_clients/n_clients*100:.1f}%)")

# === –≠–¢–ê–ü 2: –ö–û–ù–¢–†–û–õ–ò–†–£–ï–ú–´–ô SPECTRAL –î–õ–Ø –ù–ï–û–ü–†–ï–î–ï–õ–ï–ù–ù–´–• ===
print("\nüåê –≠–¢–ê–ü 2: SpectralClustering –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö...")

# –ù–∞—Ö–æ–¥–∏–º –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
uncertain_mask = gmm_max_probs < uncertainty_threshold
uncertain_count = uncertain_mask.sum()

print(
    f"üìä –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤ GMM: {uncertain_count:,} ({uncertain_count/n_clients*100:.1f}%)")

if uncertain_count > 10:  # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
    print("üîß –ü—Ä–∏–º–µ–Ω—è–µ–º SpectralClustering –∫ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–º –∫–ª–∏–µ–Ω—Ç–∞–º...")

    # –î–∞–Ω–Ω—ã–µ —Ç–æ–ª—å–∫–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
    X_uncertain = X_scaled[uncertain_mask]

    # –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è Spectral
    n_spectral_clusters = min(
        5, max(2, uncertain_count // 150))  # –†–∞–∑—É–º–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

    try:
        spectral = SpectralClustering(
            n_clusters=n_spectral_clusters,
            affinity='rbf',
            random_state=42,
            n_jobs=2  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º
        )

        spectral_labels_uncertain = spectral.fit_predict(X_uncertain)
        print(
            f"‚úÖ SpectralClustering: {n_spectral_clusters} –º–∏–∫—Ä–æ–∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö")

        # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –º–∞—Å—Å–∏–≤ –º–µ—Ç–æ–∫ Spectral
        # -1 –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö GMM –∫–ª–∏–µ–Ω—Ç–æ–≤
        spectral_labels = np.full(n_clients, -1)
        spectral_labels[uncertain_mask] = spectral_labels_uncertain

    except Exception as e:
        print(f"‚ö†Ô∏è SpectralClustering –Ω–µ —É–¥–∞–ª—Å—è: {e}")
        spectral_labels = np.full(n_clients, -1)
        n_spectral_clusters = 0
else:
    print("‚ö†Ô∏è –°–ª–∏—à–∫–æ–º –º–∞–ª–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º SpectralClustering")
    spectral_labels = np.full(n_clients, -1)
    n_spectral_clusters = 0

# === –°–û–ó–î–ê–ù–ò–ï –ì–ò–ë–†–ò–î–ù–´–• –°–ï–ì–ú–ï–ù–¢–û–í ===
print("\nüîÑ –≠–¢–ê–ü 3: –°–æ–∑–¥–∞–Ω–∏–µ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")


def create_hybrid_segments(gmm_label, spectral_label, is_uncertain):
    if is_uncertain and spectral_label != -1:
        return f"REFINED_{spectral_label}"  # –£—Ç–æ—á–Ω–µ–Ω–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã
    else:
        return f"MAIN_{gmm_label}"  # –û—Å–Ω–æ–≤–Ω—ã–µ GMM —Å–µ–≥–º–µ–Ω—Ç—ã


# –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
client_features['gmm_cluster'] = gmm_labels
client_features['gmm_max_prob'] = gmm_max_probs
client_features['gmm_uncertain'] = uncertain_mask
client_features['spectral_cluster'] = spectral_labels

# –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
for i in range(optimal_n_gmm):
    client_features[f'prob_cluster_{i}'] = gmm_probabilities[:, i]

client_features['hybrid_segment'] = [
    create_hybrid_segments(gmm, spectral, uncertain)
    for gmm, spectral, uncertain in zip(gmm_labels, spectral_labels, uncertain_mask)
]

# –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≥–∏–±—Ä–∏–¥–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏:")
segment_counts = client_features['hybrid_segment'].value_counts()

main_segments = segment_counts[segment_counts.index.str.startswith('MAIN_')]
refined_segments = segment_counts[segment_counts.index.str.startswith(
    'REFINED_')]

print(
    f"  üèõÔ∏è –û—Å–Ω–æ–≤–Ω—ã—Ö GMM —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(main_segments)} ({main_segments.sum():,} –∫–ª–∏–µ–Ω—Ç–æ–≤)")
if len(refined_segments) > 0:
    print(
        f"  üî¨ –£—Ç–æ—á–Ω–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ Spectral: {len(refined_segments)} ({refined_segments.sum():,} –∫–ª–∏–µ–Ω—Ç–æ–≤)")

# === –ü–û–õ–ù–´–ô –ê–ù–ê–õ–ò–ó GMM –ö–õ–ê–°–¢–ï–†–û–í (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) ===
print(f"\nüìà –ê–Ω–∞–ª–∏–∑ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
print("üìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º:")
cluster_sizes = pd.Series(gmm_labels).value_counts().sort_index()
for cluster_id, size in cluster_sizes.items():
    percentage = size / n_clients * 100
    avg_prob = gmm_probabilities[gmm_labels == cluster_id, cluster_id].mean()
    print(
        f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {size:,} –∫–ª–∏–µ–Ω—Ç–æ–≤ ({percentage:.1f}%), —Å—Ä.–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {avg_prob:.3f}")

# –ü—Ä–æ—Ñ–∏–ª–∏ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
print(f"\nüí° –ü—Ä–æ—Ñ–∏–ª–∏ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
key_metrics = ['total_amount', 'avg_amount', 'transaction_count', 'unique_merchants',
               'weekend_ratio', 'business_hours_ratio', 'regularity_score']

for cluster_id in sorted(pd.Series(gmm_labels).unique()):
    cluster_data = client_features[client_features['gmm_cluster'] == cluster_id]
    size = len(cluster_data)
    avg_certainty = cluster_data['gmm_max_prob'].mean()

    print(
        f"\nüîπ –ö–õ–ê–°–¢–ï–† {cluster_id} ({size:,} –∫–ª–∏–µ–Ω—Ç–æ–≤, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_certainty:.3f}):")

    for metric in key_metrics:
        if metric in cluster_data.columns:
            value = cluster_data[metric].mean()
            if 'amount' in metric:
                print(f"  ‚Ä¢ {metric}: {value:,.0f} —Ç–µ–Ω–≥–µ")
            elif 'ratio' in metric or 'score' in metric:
                print(f"  ‚Ä¢ {metric}: {value:.3f}")
            else:
                print(f"  ‚Ä¢ {metric}: {value:.1f}")

# === –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø ===
print("\nüé® –®–∞–≥ 4: –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è...")

# PCA –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# –°–æ–∑–¥–∞–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
fig.suptitle(
    '–ì–∏–±—Ä–∏–¥–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è: –ü–æ–ª–Ω—ã–π GMM + –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π SpectralClustering', fontsize=16)

# 1. GMM –∫–ª–∞—Å—Ç–µ—Ä—ã
colors_gmm = plt.cm.tab10(np.linspace(0, 1, optimal_n_gmm))
for i in range(optimal_n_gmm):
    mask = gmm_labels == i
    axes[0, 0].scatter(X_pca[mask, 0], X_pca[mask, 1],
                       c=[colors_gmm[i]], label=f'GMM {i}', alpha=0.7, s=15)
axes[0, 0].set_title(f'GMM –∫–ª–∞—Å—Ç–µ—Ä—ã ({optimal_n_gmm})')
axes[0, 0].legend()
axes[0, 0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0, 0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')

# 2. –ö–∞—Ä—Ç–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ GMM
scatter = axes[0, 1].scatter(X_pca[:, 0], X_pca[:, 1], c=gmm_max_probs,
                             cmap='viridis', alpha=0.6, s=15)
axes[0, 1].set_title('GMM: –ö–∞—Ä—Ç–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏')
axes[0, 1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0, 1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')
plt.colorbar(scatter, ax=axes[0, 1])

# 3. –ì–∏–±—Ä–∏–¥–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã (—Ç–æ–ø-10)
top_segments = segment_counts.head(10)
colors_hybrid = plt.cm.Set3(np.linspace(0, 1, len(top_segments)))
for i, segment in enumerate(top_segments.index):
    mask = client_features['hybrid_segment'] == segment
    axes[0, 2].scatter(X_pca[mask, 0], X_pca[mask, 1],
                       c=[colors_hybrid[i]], label=segment, alpha=0.7, s=15)
axes[0, 2].set_title('–ì–∏–±—Ä–∏–¥–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã (—Ç–æ–ø-10)')
axes[0, 2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
axes[0, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')
axes[0, 2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')

# 4. BIC –∫—Ä–∏–≤–∞—è GMM (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
axes[1, 0].plot(n_components_range, bic_scores, 'b-o', label='BIC')
axes[1, 0].plot(n_components_range, aic_scores, 'r-s', label='AIC')
axes[1, 0].axvline(x=optimal_n_gmm, color='green',
                   linestyle='--', label=f'–í—ã–±—Ä–∞–Ω–æ: {optimal_n_gmm}')
axes[1, 0].set_title('–ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏')
axes[1, 0].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
axes[1, 0].set_ylabel('–ó–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è')
axes[1, 0].legend()

# 5. –°–∏–ª—É—ç—Ç –∞–Ω–∞–ª–∏–∑ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
axes[1, 1].plot(n_components_range, silhouette_scores, 'g-^')
axes[1, 1].axvline(x=optimal_n_gmm, color='green', linestyle='--')
axes[1, 1].set_title('–°–∏–ª—É—ç—Ç –∞–Ω–∞–ª–∏–∑')
axes[1, 1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤')
axes[1, 1].set_ylabel('–°–∏–ª—É—ç—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç')

# 6. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
axes[1, 2].hist(gmm_max_probs, bins=30, alpha=0.7, color='skyblue')
axes[1, 2].axvline(x=uncertainty_threshold, color='red', linestyle='--',
                   label=f'–ü–æ—Ä–æ–≥ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏: {uncertainty_threshold}')
axes[1, 2].set_title('GMM: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π')
axes[1, 2].set_xlabel('–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å')
axes[1, 2].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∏–µ–Ω—Ç–æ–≤')
axes[1, 2].legend()

plt.tight_layout()
plt.show()

# === –ë–ò–ó–ù–ï–°-–ê–ù–ê–õ–ò–ó ===
print("\nüíº –®–∞–≥ 5: –ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤...")


def analyze_business_value(segment_data, segment_name):
    """–ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-—Ü–µ–Ω–Ω–æ—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞"""
    size = len(segment_data)
    total_revenue = segment_data['total_amount'].sum()
    avg_revenue = segment_data['total_amount'].mean()
    avg_transactions = segment_data['transaction_count'].mean()

    # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª —Ä–æ—Å—Ç–∞
    growth_potential = (
        segment_data['unique_merchants'].mean() * 0.3 +
        segment_data['transaction_count'].mean() * 0.4 +
        (1 - segment_data['regularity_score'].mean()) * 0.3
    )

    # –†–∏—Å–∫-–ø—Ä–æ—Ñ–∏–ª—å
    risk_score = (
        segment_data['coefficient_variation'].mean() * 0.4 +
        segment_data['weekend_ratio'].mean() * 0.2 +
        segment_data['night_ratio'].mean() * 0.4
    )

    return {
        'segment': segment_name,
        'size': size,
        'total_revenue': total_revenue,
        'avg_revenue': avg_revenue,
        'avg_transactions': avg_transactions,
        'growth_potential': growth_potential,
        'risk_score': risk_score
    }


# –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤
business_analysis = []
for segment in client_features['hybrid_segment'].unique():
    segment_data = client_features[client_features['hybrid_segment'] == segment]
    analysis = analyze_business_value(segment_data, segment)
    business_analysis.append(analysis)

business_df = pd.DataFrame(business_analysis)
business_df = business_df.sort_values('total_revenue', ascending=False)

print("üèÜ –†–µ–π—Ç–∏–Ω–≥ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ –ø–æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏:")
for _, row in business_df.head(10).iterrows():
    print(f"  {row['segment']}: {row['total_revenue']:,.0f} —Ç–µ–Ω–≥–µ "
          f"({row['size']:,} –∫–ª–∏–µ–Ω—Ç–æ–≤, {row['avg_revenue']:,.0f} —Å—Ä.–¥–æ—Ö–æ–¥)")

# === –ë–ê–ù–ö–û–í–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò ===
print(f"\nüéØ –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:")


def generate_banking_recommendations(segment_data, segment_name):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –±–∞–Ω–∫–æ–≤—Å–∫–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
    avg_amount = segment_data['total_amount'].mean()
    avg_transactions = segment_data['transaction_count'].mean()
    weekend_ratio = segment_data['weekend_ratio'].mean()
    unique_merchants = segment_data['unique_merchants'].mean()
    regularity = segment_data['regularity_score'].mean()
    gmm_certainty = segment_data['gmm_max_prob'].mean()

    recommendations = []
    segment_type = "–û—Å–Ω–æ–≤–Ω–æ–π" if segment_name.startswith(
        'MAIN_') else "–£—Ç–æ—á–Ω–µ–Ω–Ω—ã–π"

    # –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –ø–æ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏
    if avg_amount > 2000000:  # High-value
        recommendations.extend([
            "üèÜ VIP-—Å—Ç–∞—Ç—É—Å —Å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º –º–µ–Ω–µ–¥–∂–µ—Ä–æ–º",
            "üíé –ü—Ä–µ–º–∏–∞–ª—å–Ω—ã–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –∫–∞—Ä—Ç—ã (World Elite, Infinite)",
            "üè† –ò–ø–æ—Ç–µ—á–Ω—ã–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Å –ª—å–≥–æ—Ç–Ω—ã–º–∏ —Å—Ç–∞–≤–∫–∞–º–∏",
            "üìà –ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã –∏ —á–∞—Å—Ç–Ω–æ–µ –±–∞–Ω–∫–æ–≤—Å–∫–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ"
        ])
    elif avg_amount > 800000:  # Medium-high value
        recommendations.extend([
            "üí≥ –ü—Ä–µ–º–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ä—Ç—ã —Å –ø–æ–≤—ã—à–µ–Ω–Ω—ã–º –∫—ç—à–±–µ–∫–æ–º",
            "üéØ –¶–µ–ª–µ–≤—ã–µ –∫—Ä–µ–¥–∏—Ç—ã (–∞–≤—Ç–æ, –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ) —Å –ª—å–≥–æ—Ç–Ω—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏",
            "üìä –î–µ–ø–æ–∑–∏—Ç—ã –∏ –Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã",
            "üõ°Ô∏è –°—Ç—Ä–∞—Ö–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã"
        ])
    elif avg_amount > 300000:  # Medium value
        recommendations.extend([
            "üí∞ –ü—Ä–æ–≥—Ä–∞–º–º—ã –ª–æ—è–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫—ç—à–±–µ–∫",
            "üè™ –†–∞—Å—Å—Ä–æ—á–∫–∞ –≤ –ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∏—Ö –º–∞–≥–∞–∑–∏–Ω–∞—Ö",
            "üì± –ú–æ–±–∏–ª—å–Ω—ã–µ –ø–ª–∞—Ç–µ–∂–∏ –∏ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã"
        ])
    else:  # Mass market
        recommendations.extend([
            "üì≤ –ë–∞–∑–æ–≤—ã–µ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã",
            "üí° –§–∏–Ω–∞–Ω—Å–æ–≤–∞—è –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å –∏ –æ–±—É—á–∞—é—â–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã",
            "üéÅ –ú–∏–∫—Ä–æ-–±–æ–Ω—É—Å—ã –∑–∞ –∞–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ"
        ])

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    if weekend_ratio > 0.4:
        recommendations.append("üé™ Weekend-–º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")

    if unique_merchants < 5:
        recommendations.append(
            "üåê –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ç–æ—Ä–≥–æ–≤–æ–π —Å–µ—Ç–∏ –∏ –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤")

    if regularity > 0.7:
        recommendations.append("üîÑ –ê–≤—Ç–æ–ø–ª–∞—Ç–µ–∂–∏ –∏ –ø–æ–¥–ø–∏—Å–æ—á–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã")

    if segment_name.startswith('REFINED_'):
        recommendations.append(
            "üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")

    return {
        'segment': segment_name,
        'type': segment_type,
        'certainty': gmm_certainty,
        'recommendations': recommendations[:5]  # –¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    }


# –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è —Ç–æ–ø-—Å–µ–≥–º–µ–Ω—Ç–æ–≤
print("üí° –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:")
for _, row in business_df.head(8).iterrows():
    segment_name = row['segment']
    segment_data = client_features[client_features['hybrid_segment']
                                   == segment_name]
    rec_data = generate_banking_recommendations(segment_data, segment_name)

    print(f"\nüîπ {segment_name} ({row['size']:,} –∫–ª–∏–µ–Ω—Ç–æ–≤)")
    print(
        f"   –¢–∏–ø: {rec_data['type']} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å GMM: {rec_data['certainty']:.3f})")
    print(f"   –î–æ—Ö–æ–¥: {row['total_revenue']:,.0f} —Ç–µ–Ω–≥–µ")
    for i, rec in enumerate(rec_data['recommendations'], 1):
        print(f"   {i}. {rec}")

# === –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
print(f"\nüíæ –®–∞–≥ 6: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

# –û—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
output_cols = ['card_id', 'total_amount', 'avg_amount', 'transaction_count',
               'unique_merchants', 'weekend_ratio', 'regularity_score',
               'gmm_cluster', 'gmm_max_prob', 'gmm_uncertain',
               'spectral_cluster', 'hybrid_segment']

# –î–æ–±–∞–≤–ª—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ GMM (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
for i in range(optimal_n_gmm):
    output_cols.append(f'prob_cluster_{i}')

final_results = client_features[output_cols].copy()
final_results.to_csv('hybrid_full_gmm_spectral_segments.csv', index=False)
print("‚úÖ –ì–∏–±—Ä–∏–¥–Ω—ã–µ —Å–µ–≥–º–µ–Ω—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'hybrid_full_gmm_spectral_segments.csv'")

# –ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞
business_df.to_csv('hybrid_full_business_analysis.csv', index=False)
print("‚úÖ –ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'hybrid_full_business_analysis.csv'")

# –ü—Ä–æ—Ñ–∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
segment_profiles = client_features.groupby('hybrid_segment')[feature_cols].agg([
    'mean', 'std', 'median']).round(3)
segment_profiles.to_csv('hybrid_full_segment_profiles.csv')
print("‚úÖ –ü—Ä–æ—Ñ–∏–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'hybrid_full_segment_profiles.csv'")

# –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
gmm_cluster_profiles = client_features.groupby(
    'gmm_cluster')[feature_cols].agg(['mean', 'median', 'std']).round(3)
gmm_cluster_profiles.to_csv('gmm_cluster_profiles.csv')
print("‚úÖ –ü—Ä–æ—Ñ–∏–ª–∏ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'gmm_cluster_profiles.csv'")

# –°–≤–æ–¥–∫–∞ –º–æ–¥–µ–ª–∏ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
model_summary = {
    'approach': 'Hybrid Full GMM + Controlled SpectralClustering',
    'gmm_clusters': optimal_n_gmm,
    'gmm_covariance': best_cov_type,
    'gmm_bic_score': final_models[best_cov_type]['bic'],
    'gmm_aic_score': final_models[best_cov_type]['aic'],
    'gmm_log_likelihood': final_models[best_cov_type]['log_likelihood'],
    'gmm_silhouette_score': silhouette_scores[optimal_n_gmm - 2] if len(silhouette_scores) > optimal_n_gmm - 2 else 0,
    'spectral_clusters': n_spectral_clusters,
    'total_segments': len(client_features['hybrid_segment'].unique()),
    'main_segments': len(main_segments),
    'refined_segments': len(refined_segments) if len(refined_segments) > 0 else 0,
    'uncertain_clients': uncertain_clients,
    'uncertainty_percent': uncertain_clients / n_clients * 100,
    'total_clients': n_clients,
    'features_used': len(feature_cols)
}

pd.DataFrame([model_summary]).to_csv(
    'hybrid_full_model_summary.csv', index=False)
print("‚úÖ –°–≤–æ–¥–∫–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'hybrid_full_model_summary.csv'")

# === –ò–¢–û–ì–û–í–´–ô –û–¢–ß–ï–¢ ===
print(f"\nüéâ –ì–ò–ë–†–ò–î–ù–´–ô –ê–ù–ê–õ–ò–ó: –ü–û–õ–ù–´–ô GMM + –ö–û–ù–¢–†–û–õ–ò–†–£–ï–ú–´–ô SPECTRAL –ó–ê–í–ï–†–®–ï–ù!")
print("=" * 75)
print(f"üìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
print(f"‚Ä¢ –ü–æ–¥—Ö–æ–¥: –ü–æ–ª–Ω—ã–π GMM + –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–π SpectralClustering")
print(f"‚Ä¢ GMM –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {optimal_n_gmm} (–∫–æ–≤–∞—Ä–∏–∞—Ü–∏—è: {best_cov_type})")
print(f"‚Ä¢ GMM BIC score: {final_models[best_cov_type]['bic']:.0f}")
print(f"‚Ä¢ Spectral –º–∏–∫—Ä–æ–∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_spectral_clusters}")
print(
    f"‚Ä¢ –í—Å–µ–≥–æ –≥–∏–±—Ä–∏–¥–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(client_features['hybrid_segment'].unique())}")
print(f"‚Ä¢ –û—Å–Ω–æ–≤–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(main_segments)}")
print(
    f"‚Ä¢ –£—Ç–æ—á–Ω–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(refined_segments) if len(refined_segments) > 0 else 0}")
print(f"‚Ä¢ –ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤: {uncertain_clients:,}")

print(f"\nüöÄ –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê –ì–ò–ë–†–ò–î–ù–û–ì–û –ü–û–î–•–û–î–ê:")
print("‚Ä¢ –ü–æ–ª–Ω—ã–π GMM –∞–Ω–∞–ª–∏–∑ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –≤—ã–±–æ—Ä–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
print("‚Ä¢ SpectralClustering —Ç–æ–ª—å–∫–æ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤")
print("‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
print("‚Ä¢ –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
print("‚Ä¢ –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞")

print(f"\nüíº –ë–ò–ó–ù–ï–°-–¶–ï–ù–ù–û–°–¢–¨:")
top_segment = business_df.iloc[0]
print(f"‚Ä¢ –°–∞–º—ã–π –¥–æ—Ö–æ–¥–Ω—ã–π —Å–µ–≥–º–µ–Ω—Ç: {top_segment['segment']}")
print(f"‚Ä¢ –î–æ—Ö–æ–¥ —Ç–æ–ø-—Å–µ–≥–º–µ–Ω—Ç–∞: {top_segment['total_revenue']:,.0f} —Ç–µ–Ω–≥–µ")
print(f"‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥ –∫–ª–∏–µ–Ω—Ç–∞: {top_segment['avg_revenue']:,.0f} —Ç–µ–Ω–≥–µ")
print(
    f"‚Ä¢ GMM –∫–∞—á–µ—Å—Ç–≤–æ: BIC={final_models[best_cov_type]['bic']:.0f}, —Å–∏–ª—É—ç—Ç={silhouette_scores[optimal_n_gmm - 2] if len(silhouette_scores) > optimal_n_gmm - 2 else 'N/A'}")

if len(refined_segments) > 0:
    refined_revenue = client_features[client_features['hybrid_segment'].str.startswith(
        'REFINED_')]['total_amount'].sum()
    print(f"‚Ä¢ –î–æ—Ö–æ–¥ –æ—Ç —É—Ç–æ—á–Ω–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {refined_revenue:,.0f} —Ç–µ–Ω–≥–µ")

print(f"\nüéØ –ì–û–¢–û–í–û –î–õ–Ø –ë–ê–ù–ö–û–í–°–ö–û–ì–û –í–ù–ï–î–†–ï–ù–ò–Ø:")
print("‚Ä¢ –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –±–∞–Ω–∫–æ–≤—Å–∫–∏–µ –ø—Ä–æ–¥—É–∫—Ç—ã –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º")
print("‚Ä¢ –¢–∞—Ä–≥–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Ä–∫–µ—Ç–∏–Ω–≥ –¥–ª—è –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –∫–ª–∏–µ–Ω—Ç–æ–≤")
print("‚Ä¢ VIP-–ø—Ä–æ–≥—Ä–∞–º–º—ã –¥–ª—è –≤—ã—Å–æ–∫–æ–¥–æ—Ö–æ–¥–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
print("‚Ä¢ –î–µ—Ç–∞–ª—å–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–Ω—ã—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤")
print("‚Ä¢ –ü–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –≤–Ω–µ–¥—Ä–µ–Ω–∏—è")

print(f"\n‚ú® –õ—É—á—à–µ–µ –∏–∑ –¥–≤—É—Ö –º–∏—Ä–æ–≤: –∫–∞—á–µ—Å—Ç–≤–æ GMM + —Ç–æ—á–Ω–æ—Å—Ç—å SpectralClustering!")
print("=" * 75)

# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏
gc.collect()
